/*

  Reads the content of a Kafka topic ( avro record using oasproc schema ) and dump it into HDFS file
  Verification done via avro binaries & hive external table

  Kafka topic: avrotest on beosbd01:9092 (sandbox)
  Kafka messages generated by "PushOasproc" class

-- check if file has been created
[hadoop@namenode-11 hadoop]$ hdfs dfs -ls /tmp/fpavro/*.avro
-rw-r--r--   3 hadoop supergroup       5959 2019-05-31 09:13 /tmp/fpdata.avro

-- get the file out of HDFS to check content
[hadoop@namenode-11 hadoop]$ hdfs dfs -get /tmp/fpavro/fpdata.avro ./fpdata.avro

-- will use avro binaries to check content

[root@beosbd01 avro]# pwd
/opt/fp/avro

-- get the file out of namenode-11 machine
[root@beosbd01 avro]# docker -H beoast11 cp namenode-11:/usr/share/hadoop/fpdata.avro .

[root@beosbd01 avro]# ls -lrt
total 35516
-rw-r--r--. 1 root root  1556863 May 31 08:04 avro-1.8.2.jar
-rw-r--r--. 1 root root 34798932 May 31 08:04 avro-tools-1.8.2.jar
-rw-r--r--. 1 root root     5959 May 31 09:15 fpdata.avro
[root@beosbd01 avro]#


-- start a java container to be able to run avro binaries
[root@beosbd01 avro]# docker run -it --rm --name fpolet-java --volume $PWD:/data:rw docker.swift.com:5000/java:openjdk-8-jdk
2019-05-31_09:15:51 | entrypoint | Invoking: /etc/entrypoint.d/00ca-certificates ...
2019-05-31_09:15:51 | 00ca-certificates | Configuring 00ca-certificates for first run ...
2019-05-31_09:15:51 | 00ca-certificates | Updating trusted CA certificates ...
2019-05-31_09:15:52 | entrypoint | Launching: /bin/bash ...

-- check avro schema from file
[root@56d476215174 /]# java -jar /data/avro-tools-1.8.2.jar getschema /data/fpdata.avro
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
{
  "type" : "record",
  "name" : "OasprocEvent",
  "namespace" : "com.swift.oasproc.message.avro",
  "fields" : [ {
    "name" : "version",
    "type" : "string"
  }, {
    "name" : "hostname",
    "type" : "string"
  }, {
    "name" : "application_name",
    "type" : "string"
  }, {
    "name" : "log_type",
    "type" : "string"
  }, {
    "name" : "site",
    "type" : "string"
  }, {
    "name" : "uuid",
    "type" : "string"
  }, {
    "name" : "file_path",
    "type" : "string"
  }, {
    "name" : "file_name",
    "type" : "string"
  }, {
    "name" : "offset",
    "type" : "string"
  }, {
    "name" : "message",
    "type" : "string"
  }, {
    "name" : "timestamp",
    "type" : "string"
  }, {
    "name" : "logagent_timestamp",
    "type" : "string"
  } ]
}

-- get file content
[root@56d476215174 /]# java -jar /data/avro-tools-1.8.2.jar tojson /data/fpdata.avro
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"7","message":"{ \"age\":7, \"name\":\"john7\", \"message\":\"hello7\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"10","message":"{ \"age\":10, \"name\":\"john10\", \"message\":\"hello10\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"27","message":"{ \"age\":27, \"name\":\"john27\", \"message\":\"hello27\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"14","message":"{ \"age\":14, \"name\":\"john14\", \"message\":\"hello14\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"16","message":"{ \"age\":16, \"name\":\"john16\", \"message\":\"hello16\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"24","message":"{ \"age\":24, \"name\":\"john24\", \"message\":\"hello24\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"1","message":"{ \"age\":1, \"name\":\"john1\", \"message\":\"hello1\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"15","message":"{ \"age\":15, \"name\":\"john15\", \"message\":\"hello15\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"17","message":"{ \"age\":17, \"name\":\"john17\", \"message\":\"hello17\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"21","message":"{ \"age\":21, \"name\":\"john21\", \"message\":\"hello21\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"3","message":"{ \"age\":3, \"name\":\"john3\", \"message\":\"hello3\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"9","message":"{ \"age\":9, \"name\":\"john9\", \"message\":\"hello9\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"29","message":"{ \"age\":29, \"name\":\"john29\", \"message\":\"hello29\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"6","message":"{ \"age\":6, \"name\":\"john6\", \"message\":\"hello6\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"11","message":"{ \"age\":11, \"name\":\"john11\", \"message\":\"hello11\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"20","message":"{ \"age\":20, \"name\":\"john20\", \"message\":\"hello20\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"23","message":"{ \"age\":23, \"name\":\"john23\", \"message\":\"hello23\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"5","message":"{ \"age\":5, \"name\":\"john5\", \"message\":\"hello5\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"8","message":"{ \"age\":8, \"name\":\"john8\", \"message\":\"hello8\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"22","message":"{ \"age\":22, \"name\":\"john22\", \"message\":\"hello22\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"25","message":"{ \"age\":25, \"name\":\"john25\", \"message\":\"hello25\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"4","message":"{ \"age\":4, \"name\":\"john4\", \"message\":\"hello4\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"13","message":"{ \"age\":13, \"name\":\"john13\", \"message\":\"hello13\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"18","message":"{ \"age\":18, \"name\":\"john18\", \"message\":\"hello18\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"19","message":"{ \"age\":19, \"name\":\"john19\", \"message\":\"hello19\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"12","message":"{ \"age\":12, \"name\":\"john12\", \"message\":\"hello12\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"2","message":"{ \"age\":2, \"name\":\"john2\", \"message\":\"hello2\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"26","message":"{ \"age\":26, \"name\":\"john26\", \"message\":\"hello26\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
{"version":"1.0","hostname":"beodemo01","application_name":"demo","log_type":"logtype-test","site":"siteA","uuid":"123456789","file_path":"/test/data","file_name":"demo.txt","offset":"28","message":"{ \"age\":28, \"name\":\"john28\", \"message\":\"hello28\" }","timestamp":"2019-02-04 08:00:00.00","logagent_timestamp":"2019-01-05 10:25:30.23"}
[root@56d476215174 /]#

-- put avro schema in HDFS
[hadoop@namenode-11 hadoop]$ more oasproc.avsc
{
  "type": "record",
  "namespace": "com.swift.oasproc.message.avro",
  "name": "OasprocEvent",
  "fields": [
    { "name": "version", "type": "string" },
    { "name": "hostname", "type": "string" },
    { "name": "application_name", "type": "string" },
    { "name": "log_type", "type": "string" },
    { "name": "site", "type": "string" },
    { "name": "uuid", "type": "string" },
    { "name": "file_path", "type": "string" },
    { "name": "file_name", "type": "string" },
    { "name": "offset", "type": "string" },
    { "name": "message", "type": "string" },
    { "name": "timestamp", "type": "string"},
    { "name": "logagent_timestamp", "type": "string"}
  ]
}
[hadoop@namenode-11 hadoop]$ hdfs dfs -put oasproc.avsc /tmp/oasproc.avsc
[hadoop@namenode-11 hadoop]$

-- create external table in Hive
[hive@hive-hiveserver2-2 hive]$ hive
which: no hbase in (/opt/hive/bin:/usr/share/hadoop/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin)
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/share/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

Logging initialized using configuration in file:/opt/hive/conf/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
hive> CREATE EXTERNAL TABLE avro_oasproc
    > ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
    > STORED as INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
    > OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
    > LOCATION '/tmp/fpavro'
    > TBLPROPERTIES ('avro.schema.url'='hdfs://beoast11:8020/tmp/oasproc.avsc');
OK
Time taken: 1.366 seconds
hive>


    > select * from avro_oasproc;
OK
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        7       { "age":7, "name":"john7", "message":"hello7" } 2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        10      { "age":10, "name":"john10", "message":"hello10" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        27      { "age":27, "name":"john27", "message":"hello27" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        14      { "age":14, "name":"john14", "message":"hello14" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        16      { "age":16, "name":"john16", "message":"hello16" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        24      { "age":24, "name":"john24", "message":"hello24" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        1       { "age":1, "name":"john1", "message":"hello1" } 2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        15      { "age":15, "name":"john15", "message":"hello15" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        17      { "age":17, "name":"john17", "message":"hello17" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        21      { "age":21, "name":"john21", "message":"hello21" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        3       { "age":3, "name":"john3", "message":"hello3" } 2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        9       { "age":9, "name":"john9", "message":"hello9" } 2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        29      { "age":29, "name":"john29", "message":"hello29" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        6       { "age":6, "name":"john6", "message":"hello6" } 2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        11      { "age":11, "name":"john11", "message":"hello11" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        20      { "age":20, "name":"john20", "message":"hello20" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        23      { "age":23, "name":"john23", "message":"hello23" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        5       { "age":5, "name":"john5", "message":"hello5" } 2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        8       { "age":8, "name":"john8", "message":"hello8" } 2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        22      { "age":22, "name":"john22", "message":"hello22" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        25      { "age":25, "name":"john25", "message":"hello25" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        4       { "age":4, "name":"john4", "message":"hello4" } 2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        13      { "age":13, "name":"john13", "message":"hello13" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        18      { "age":18, "name":"john18", "message":"hello18" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        19      { "age":19, "name":"john19", "message":"hello19" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        12      { "age":12, "name":"john12", "message":"hello12" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        2       { "age":2, "name":"john2", "message":"hello2" } 2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        26      { "age":26, "name":"john26", "message":"hello26" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
1.0     beodemo01       demo    logtype-test    siteA   123456789       /test/data      demo.txt        28      { "age":28, "name":"john28", "message":"hello28" }      2019-02-04 08:00:00.00  2019-01-05 10:25:30.23
Time taken: 1.654 seconds, Fetched: 29 row(s)
hive>
hive>
    > desc avro_oasproc;
OK
version                 string
hostname                string
application_name        string
log_type                string
site                    string
uuid                    string
file_path               string
file_name               string
offset                  string
message                 string
timestamp               string
logagent_timestamp      string
Time taken: 0.036 seconds, Fetched: 12 row(s)
hive>
hive> select message, offset from avro_oasproc;
OK
{ "age":7, "name":"john7", "message":"hello7" } 7
{ "age":10, "name":"john10", "message":"hello10" }      10
{ "age":27, "name":"john27", "message":"hello27" }      27
{ "age":14, "name":"john14", "message":"hello14" }      14
{ "age":16, "name":"john16", "message":"hello16" }      16
{ "age":24, "name":"john24", "message":"hello24" }      24
{ "age":1, "name":"john1", "message":"hello1" } 1
{ "age":15, "name":"john15", "message":"hello15" }      15
{ "age":17, "name":"john17", "message":"hello17" }      17
{ "age":21, "name":"john21", "message":"hello21" }      21
{ "age":3, "name":"john3", "message":"hello3" } 3
{ "age":9, "name":"john9", "message":"hello9" } 9
{ "age":29, "name":"john29", "message":"hello29" }      29
{ "age":6, "name":"john6", "message":"hello6" } 6
{ "age":11, "name":"john11", "message":"hello11" }      11
{ "age":20, "name":"john20", "message":"hello20" }      20
{ "age":23, "name":"john23", "message":"hello23" }      23
{ "age":5, "name":"john5", "message":"hello5" } 5
{ "age":8, "name":"john8", "message":"hello8" } 8
{ "age":22, "name":"john22", "message":"hello22" }      22
{ "age":25, "name":"john25", "message":"hello25" }      25
{ "age":4, "name":"john4", "message":"hello4" } 4
{ "age":13, "name":"john13", "message":"hello13" }      13
{ "age":18, "name":"john18", "message":"hello18" }      18
{ "age":19, "name":"john19", "message":"hello19" }      19
{ "age":12, "name":"john12", "message":"hello12" }      12
{ "age":2, "name":"john2", "message":"hello2" } 2
{ "age":26, "name":"john26", "message":"hello26" }      26
{ "age":28, "name":"john28", "message":"hello28" }      28
Time taken: 0.172 seconds, Fetched: 29 row(s)
hive>


*/

package com.swift.oasis.copyKafkaTopic.run;

import com.swift.oasis.copyKafkaTopic.library.KafkaUtils;
import org.apache.avro.Schema;
import org.apache.avro.file.DataFileWriter;
import org.apache.avro.generic.GenericDatumWriter;
import org.apache.avro.generic.GenericRecord;
import org.apache.avro.io.BinaryDecoder;
import org.apache.avro.io.DatumWriter;
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.serialization.ByteArrayDeserializer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.io.OutputStream;
import java.net.URI;
import java.util.Collections;
import java.util.Properties;
import java.util.UUID;

public class TopicDumpAvroHdfs {

    class HDFSConnection {
        public DataFileWriter df;
        public OutputStream out;

        HDFSConnection (DataFileWriter p_df, OutputStream p_out)
        {
            this.out = p_out;
            this.df = p_df;
        }
    }

    final private static String KafkaServerName = "beosbd01:9092";
    final private static String ZookeeperServerName = "beosbd01:2181";

    //-----------------------------------------------------------------------------

    public static void main(final String[] args) {

        long startTime = System.currentTimeMillis();
        final Logger LOGGER = LoggerFactory.getLogger(KafkaUtils.class);
        final KafkaUtils kafkaUtils = new KafkaUtils(KafkaServerName, ZookeeperServerName);

        TopicDumpAvroHdfs test = new TopicDumpAvroHdfs();
        Schema schema = test.parseSchema();
        HDFSConnection connection = test.openHDFSFile (schema);
        test.readDataFromKafka("avrotest", "earliest", schema, connection);
        test.closeHDFSFile(connection);
    }

    //-----------------------------------------------------------------------------

    public void readDataFromKafka(final String outputTopic, final String mode, final Schema schema, final HDFSConnection connection ) {
        boolean stop = false;
        int norec_count = 0;
        final Properties props = new Properties();
        String group_id = UUID.randomUUID().toString();
        props.put("bootstrap.servers", KafkaServerName);
        props.put("group.id", group_id); //System.getProperty("user.name")+"-151");
        props.put("auto.offset.reset", mode);
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", ByteArrayDeserializer.class.getName());

        final KafkaConsumer<String, byte[]> consumer = new KafkaConsumer<String, byte[]>(props);
        //ensureTopicExistence(outputTopic);
        consumer.subscribe(Collections.singletonList(outputTopic));

        try {
            int i = 0;
            while (stop == false) {
                final ConsumerRecords<String, byte[]> records = consumer.poll(100);
                for (final ConsumerRecord<String, byte[]> record : records) {
                    System.out.println("key : [" + record.key() +"] offset : " + record.offset() + " partition : " + record.partition()+ " data : " +new String(record.value()));
                    byte [] data = record.value();
                    this.writetoAvroToHDFS(connection.df,schema,data);
                    norec_count = 0;
                }
                norec_count++;
                System.out.println( "norec_count = " + norec_count);
                if (norec_count == 100) stop = true;

            }
        } finally {
            consumer.close();
            System.out.println("End of queue reached");
        }
    }

    //-----------------------------------------------------------------------------

    // parsing the schema
    private Schema parseSchema() {
        Schema.Parser parser = new Schema.Parser();
        Schema schema = null;
        String schemaData = "{\n" +
                "  \"type\": \"record\",\n" +
                "  \"namespace\": \"com.swift.oasproc.message.avro\",\n" +
                "  \"name\": \"OasprocEvent\",\n" +
                "  \"fields\": [\n" +
                "    { \"name\": \"version\", \"type\": \"string\" },\n" +
                "    { \"name\": \"hostname\", \"type\": \"string\" },\n" +
                "    { \"name\": \"application_name\", \"type\": \"string\" },\n" +
                "    { \"name\": \"log_type\", \"type\": \"string\" },\n" +
                "    { \"name\": \"site\", \"type\": \"string\" },\n" +
                "    { \"name\": \"uuid\", \"type\": \"string\" },\n" +
                "    { \"name\": \"file_path\", \"type\": \"string\" },\n" +
                "    { \"name\": \"file_name\", \"type\": \"string\" },\n" +
                "    { \"name\": \"offset\", \"type\": \"string\" },\n" +
                "    { \"name\": \"message\", \"type\": \"string\" },\n" +
                "    { \"name\": \"timestamp\", \"type\": \"string\"},\n" +
                "    { \"name\": \"logagent_timestamp\", \"type\": \"string\"}\n" +
                "  ]\n" +
                "}\n";
        try {
            // Path to schema file
            schema = parser.parse(schemaData);

        } catch (Exception e) {
            e.printStackTrace();
        }
        return schema;
    }

    //-----------------------------------------------------------------------------

    private HDFSConnection openHDFSFile (Schema schema)
    {
        String path = "hdfs://beoast11:8020/tmp/fpavro";
        String fileName = "fpdata.avro";
        String hdfsuri = path+"/"+fileName;
        DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<GenericRecord>(schema);
        DataFileWriter<GenericRecord> dataFileWriter = null;
       OutputStream out = null;

        try {
            //out file path in HDFS
            Configuration conf = new Configuration();
            // Set FileSystem URI
            conf.set("fs.defaultFS", hdfsuri);
            // Because of Maven
            conf.set("fs.hdfs.impl", org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());
            conf.set("fs.file.impl", org.apache.hadoop.fs.LocalFileSystem.class.getName());
            // Set HADOOP user
            System.setProperty("HADOOP_USER_NAME", "hadoop");
            System.setProperty("hadoop.home.dir", "/");

            // support append
            conf.setBoolean("dfs.support.append", true);

            //Get the filesystem - HDFS
            URI uri = URI.create(hdfsuri);
            Path hdfspath = new Path(hdfsuri);
            FileSystem fs = FileSystem.get(uri, conf);
            //OutputStream out = null;

            if (!fs.exists(hdfspath))
            {
                System.out.println("Creating "+hdfspath);
                out = fs.create(hdfspath);
            }
            else
            {
                System.out.println("Appending "+hdfspath);
                out = fs.append(hdfspath);
            }

            dataFileWriter = new DataFileWriter<GenericRecord>(datumWriter);
            // for compression
            //dataFileWriter.setCodec(CodecFactory.snappyCodec());
            dataFileWriter.create(schema, out);

        } catch (IOException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }
        return new HDFSConnection(dataFileWriter,out);
    }

    //-----------------------------------------------------------------------------

    private void closeHDFSFile (HDFSConnection connection)
    {
        if(connection != null) {
            try {
                connection.df.close();
                connection.out.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }

    //-----------------------------------------------------------------------------

    private void writetoAvroToHDFS(DataFileWriter<GenericRecord> dataFileWriter,Schema schema, byte[] data)
    {
        GenericRecord record = null;

        // !!! TO DO - improve to not recreate decoder each time !!
        try {
            SpecificDatumReader<GenericRecord> reader = new SpecificDatumReader<GenericRecord>(schema);
            BinaryDecoder binaryDecoder = DecoderFactory.get().binaryDecoder(data,null);
            record = reader.read(null, binaryDecoder);

            dataFileWriter.append(record);
        }
        catch (Exception e)
        {
            e.printStackTrace();
        }
    }

    //-----------------------------------------------------------------------------
}

 

Pushoasproc (rely on oasis-lib)

 

package com.swift.oasis.copyKafkaTopic.run;

import com.swift.oasis.avro.AvroSerializer;
import com.swift.oasproc.message.avro.OasprocEvent;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.ByteArraySerializer;
import org.apache.kafka.common.serialization.StringSerializer;

import java.util.Properties;

public class PushOasproc {
    public static void main(final String[] args) {

        final String kafka_server = "beosbd01:9092";
        final String outputTopic = "avrotest";

        // Initialize producer
        final Properties props_producer = new Properties();
        props_producer.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka_server);
        props_producer.put(ProducerConfig.CLIENT_ID_CONFIG, System.getProperty("user.name"));
        props_producer.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props_producer.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());
        props_producer.put(ProducerConfig.LINGER_MS_CONFIG, "100");
        props_producer.put(ProducerConfig.BATCH_SIZE_CONFIG, "10000");

        for (int i = 1; i  < 30; i++) {
            OasprocEvent event = new OasprocEvent();
            event.setApplicationName("demo");
            event.setFileName("demo.txt");
            event.setHostname("beodemo01");
            event.setLogagentTimestamp("2019-01-05 10:25:30.23");
            event.setFilePath("/test/data");
            event.setLogType("logtype-test");
            event.setOffset(""+i);
            event.setTimestamp("2019-02-04 08:00:00.00");
            event.setUuid("123456789");
            event.setVersion("1.0");
            event.setMessage("{ \"age\":"+i+", \"name\":\"john"+i+"\", \"message\":\"hello"+i+"\" }");
            event.setSite("siteA");

            try {
                System.out.println (" push "+i);
                AvroSerializer<OasprocEvent> serializer = new AvroSerializer(OasprocEvent.SCHEMA$);
                byte[] data = serializer.serialize(event);

                final Producer<String, byte[]> producer = new KafkaProducer<>(props_producer);

                final ProducerRecord<String, byte[]> pr = new ProducerRecord<>(outputTopic, data);
                producer.send(pr);
                producer.close();
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
    }
}

